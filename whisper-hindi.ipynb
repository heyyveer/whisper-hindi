{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14152401,"sourceType":"datasetVersion","datasetId":9020104},{"sourceId":14160433,"sourceType":"datasetVersion","datasetId":9025542},{"sourceId":685696,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":520117,"modelId":534428},{"sourceId":684284,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":519197,"modelId":533658}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/veertiiiiwari/whisper-hindi?scriptVersionId=286266354\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:04:18.157411Z","iopub.execute_input":"2025-12-15T05:04:18.157777Z","iopub.status.idle":"2025-12-15T05:04:25.006335Z","shell.execute_reply.started":"2025-12-15T05:04:18.157747Z","shell.execute_reply":"2025-12-15T05:04:25.005512Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"# from youtube","metadata":{}},{"cell_type":"code","source":"!pip install yt-dlp\n!yt-dlp -x --audio-format wav \"https://www.youtube.com/watch?v=sGAe5K79h-g\" -o \"raw.wav\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# single audio","metadata":{}},{"cell_type":"code","source":"!pip install pydub\n\nfrom pydub import AudioSegment, silence\nimport os\n##### from youtube audio ######\n#audio = AudioSegment.from_wav(\"raw.wav\")\n\n##### my recorded audio ######\naudio = AudioSegment.from_wav(\"/kaggle/input/recording1/1.wav\")\n\nchunks = silence.split_on_silence(\n    audio,\n    min_silence_len=1000,   # silence >= 700ms\n    silence_thresh=-40      # dBFS threshold\n)\n\nos.makedirs(\"wavs\", exist_ok=True)\n\nfor i, chunk in enumerate(chunks):\n    print(i)\n    out = f\"wavs/utt_{i:04d}.wav\"\n    chunk.export(out, format=\"wav\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multiple Audios ","metadata":{}},{"cell_type":"code","source":"!pip install pydub\n\nfrom pydub import AudioSegment, silence\nimport os\nfrom glob import glob\n\n# Input folder containing many wav files\nINPUT_DIR = \"/kaggle/input/whisper-hindi-dataset-h4-ankit/whisper_hinid_dataset_h4_ankit\"\n\n# Output folder\nOUTPUT_DIR = \"wavs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Get list of all WAV files inside dataset\nwav_files = glob(os.path.join(INPUT_DIR, \"*.wav\"))\nprint(\"Total WAV files found:\", len(wav_files))\n\nchunk_counter = 0   # global counter to keep slices unique\n\nfor file_path in wav_files:\n    print(\"Processing:\", file_path)\n\n    # Load audio\n    audio = AudioSegment.from_wav(file_path)\n\n    # Split into chunks based on silence\n    chunks = silence.split_on_silence(\n        audio,\n        min_silence_len=1000,   # adjust if needed\n        silence_thresh=-40      # dBFS threshold\n    )\n\n    # Save chunks\n    for chunk in chunks:\n        out_path = os.path.join(OUTPUT_DIR, f\"utt_{chunk_counter:05d}.wav\")\n        chunk.export(out_path, format=\"wav\")\n        chunk_counter += 1\n\nprint(\"Done! Total chunks saved:\", chunk_counter)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:58:15.063224Z","iopub.execute_input":"2025-12-06T13:58:15.063906Z","iopub.status.idle":"2025-12-06T14:02:19.732816Z","shell.execute_reply.started":"2025-12-06T13:58:15.063876Z","shell.execute_reply":"2025-12-06T14:02:19.731949Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transcripts from whisper v3 large hindi","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/openai/whisper.git","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-12-14T08:31:21.063831Z","iopub.execute_input":"2025-12-14T08:31:21.064201Z","iopub.status.idle":"2025-12-14T08:31:22.010273Z","shell.execute_reply.started":"2025-12-14T08:31:21.064161Z","shell.execute_reply":"2025-12-14T08:31:22.009041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import whisper\nimport os\n\n# Load the multilingual small model (supports Hindi)\nmodel = whisper.load_model(\"large\")\n\nmetadata = []\n\nfor f in sorted(os.listdir(\"wavs\")):\n    if f.endswith(\".wav\"):\n        result = model.transcribe(f\"wavs/{f}\", language=\"hi\")\n        text = result[\"text\"].strip()\n        print(text)\n        metadata.append([f, text])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transcripts from whisper collabora hindi","metadata":{}},{"cell_type":"code","source":"!pip install transformers torchaudio sentencepiece accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T11:58:01.197927Z","iopub.execute_input":"2025-12-14T11:58:01.198485Z","iopub.status.idle":"2025-12-14T12:03:15.033736Z","shell.execute_reply.started":"2025-12-14T11:58:01.198439Z","shell.execute_reply":"2025-12-14T12:03:15.032407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load Hindi fine-tuned Whisper Large v3\nprocessor = WhisperProcessor.from_pretrained(\"collabora/whisper-large-v2-hindi\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\n    \"collabora/whisper-large-v2-hindi\"\n).to(device)\nmodel.eval()\n\nmetadata = []\n\ndef transcribe(path):\n    # Load audio\n    audio, sr = torchaudio.load(path)\n\n    # Resample to 16k\n    if sr != 16000:\n        audio = torchaudio.functional.resample(audio, sr, 16000)\n\n    # Preprocess\n    inputs = processor(\n        audio.squeeze().numpy(),\n        sampling_rate=16000,\n        return_tensors=\"pt\"\n    ).to(device)\n\n    # Predict\n    pred_ids = model.generate(inputs[\"input_features\"])\n\n    # Decode\n    text = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n    return text.strip()\n\n\n# -------------------------\n# Transcription Loop\n# -------------------------\ncount = 0\nfor f in sorted(os.listdir(\"wavs\")):\n    if f.endswith(\".wav\"):\n        count = count + 1\n        text = transcribe(f\"wavs/{f}\")\n        print(count)\n        #print(text)\n        metadata.append([f, text])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:05:43.932932Z","iopub.execute_input":"2025-12-06T14:05:43.933429Z","iopub.status.idle":"2025-12-06T14:22:48.102909Z","shell.execute_reply.started":"2025-12-06T14:05:43.933401Z","shell.execute_reply":"2025-12-06T14:22:48.102108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Hindi number map (0‚Äì100)\nhindi_numbers = {\n    0:\"‡§∂‡•Ç‡§®‡•ç‡§Ø\", 1:\"‡§è‡§ï\", 2:\"‡§¶‡•ã\", 3:\"‡§§‡•Ä‡§®\", 4:\"‡§ö‡§æ‡§∞\", 5:\"‡§™‡§æ‡§Ç‡§ö\", 6:\"‡§õ‡§π\", 7:\"‡§∏‡§æ‡§§\", 8:\"‡§Ü‡§†\", 9:\"‡§®‡•å\",\n    10:\"‡§¶‡§∏\", 11:\"‡§ó‡•ç‡§Ø‡§æ‡§∞‡§π\", 12:\"‡§¨‡§æ‡§∞‡§π\", 13:\"‡§§‡•á‡§∞‡§π\", 14:\"‡§ö‡•å‡§¶‡§π\", 15:\"‡§™‡§Ç‡§¶‡•ç‡§∞‡§π\", 16:\"‡§∏‡•ã‡§≤‡§π\", 17:\"‡§∏‡§§‡•ç‡§∞‡§π\", 18:\"‡§Ö‡§ü‡•ç‡§†‡§æ‡§∞‡§π\", 19:\"‡§â‡§®‡•ç‡§®‡•Ä‡§∏\",\n    20:\"‡§¨‡•Ä‡§∏\", 21:\"‡§á‡§ï‡•ç‡§ï‡•Ä‡§∏\", 22:\"‡§¨‡§æ‡§á‡§∏\", 23:\"‡§§‡•á‡§à‡§∏\", 24:\"‡§ö‡•å‡§¨‡•Ä‡§∏\", 25:\"‡§™‡§ö‡•ç‡§ö‡•Ä‡§∏\", 26:\"‡§õ‡§¨‡•ç‡§¨‡•Ä‡§∏\", 27:\"‡§∏‡§§‡•ç‡§§‡§æ‡§à‡§∏\", 28:\"‡§Ö‡§ü‡•ç‡§†‡§æ‡§à‡§∏\", 29:\"‡§â‡§®‡§§‡•Ä‡§∏\",\n    30:\"‡§§‡•Ä‡§∏\", 31:\"‡§á‡§ï‡§§‡•Ä‡§∏\", 32:\"‡§¨‡§§‡•ç‡§§‡•Ä‡§∏\", 33:\"‡§§‡•à‡§Ç‡§§‡•Ä‡§∏\", 34:\"‡§ö‡•å‡§Ç‡§§‡•Ä‡§∏\", 35:\"‡§™‡•à‡§Ç‡§§‡•Ä‡§∏\", 36:\"‡§õ‡§§‡•ç‡§§‡•Ä‡§∏\", 37:\"‡§∏‡•à‡§Ç‡§§‡•Ä‡§∏\", 38:\"‡§Ö‡§°‡§º‡§§‡•Ä‡§∏\", 39:\"‡§â‡§®‡§§‡§æ‡§≤‡•Ä‡§∏\",\n    40:\"‡§ö‡§æ‡§≤‡•Ä‡§∏\", 41:\"‡§á‡§ï‡§§‡§æ‡§≤‡•Ä‡§∏\", 42:\"‡§¨‡§Ø‡§æ‡§≤‡•Ä‡§∏\", 43:\"‡§§‡•à‡§Ç‡§§‡§æ‡§≤‡•Ä‡§∏\", 44:\"‡§ö‡§µ‡§æ‡§≤‡•Ä‡§∏\", 45:\"‡§™‡•à‡§Ç‡§§‡§æ‡§≤‡•Ä‡§∏\", 46:\"‡§õ‡§ø‡§Ø‡§æ‡§≤‡•Ä‡§∏\", 47:\"‡§∏‡•à‡§Ç‡§§‡§æ‡§≤‡•Ä‡§∏\", 48:\"‡§Ö‡§°‡§º‡§§‡§æ‡§≤‡•Ä‡§∏\", 49:\"‡§â‡§®‡§ö‡§æ‡§∏\",\n    50:\"‡§™‡§ö‡§æ‡§∏\", 51:\"‡§á‡§ï‡•ç‡§Ø‡§æ‡§µ‡§®\", 52:\"‡§¨‡§æ‡§µ‡§®\", 53:\"‡§§‡§ø‡§∞‡•á‡§™‡§®\", 54:\"‡§ö‡•å‡§µ‡§®\", 55:\"‡§™‡§ö‡§™‡§®\", 56:\"‡§õ‡§™‡•ç‡§™‡§®\", 57:\"‡§∏‡§§‡•ç‡§§‡§æ‡§µ‡§®\", 58:\"‡§Ö‡§ü‡•ç‡§†‡§æ‡§µ‡§®\", 59:\"‡§â‡§®‡§∏‡§†\",\n    60:\"‡§∏‡§æ‡§†\", 61:\"‡§á‡§ï‡§∏‡§†\", 62:\"‡§¨‡§æ‡§∏‡§†\", 63:\"‡§§‡§ø‡§∞‡•á‡§∏‡§†\", 64:\"‡§ö‡•å‡§Ç‡§∏‡§†\", 65:\"‡§™‡•à‡§Ç‡§∏‡§†\", 66:\"‡§õ‡§ø‡§Ø‡§æ‡§∏‡§†\", 67:\"‡§∏‡§°‡§º‡§∏‡§†\", 68:\"‡§Ö‡§°‡§º‡§∏‡§†\", 69:\"‡§â‡§®‡§π‡§§‡•ç‡§§‡§∞\",\n    70:\"‡§∏‡§§‡•ç‡§§‡§∞\", 71:\"‡§á‡§ï‡§π‡§§‡•ç‡§§‡§∞\", 72:\"‡§¨‡§π‡§§‡•ç‡§§‡§∞\", 73:\"‡§§‡§ø‡§π‡§§‡•ç‡§§‡§∞\", 74:\"‡§ö‡•å‡§π‡§§‡•ç‡§§‡§∞\", 75:\"‡§™‡§ö‡§π‡§§‡•ç‡§§‡§∞\", 76:\"‡§õ‡§ø‡§π‡§§‡•ç‡§§‡§∞\", 77:\"‡§∏‡§§‡§π‡§§‡•ç‡§§‡§∞\", 78:\"‡§Ö‡§†‡§π‡§§‡•ç‡§§‡§∞\", 79:\"‡§â‡§®‡•ç‡§Ø‡§æ‡§∏‡•Ä\",\n    80:\"‡§Ö‡§∏‡•ç‡§∏‡•Ä\", 81:\"‡§á‡§ï‡•ç‡§Ø‡§æ‡§∏‡•Ä\", 82:\"‡§¨‡§Ø‡§æ‡§∏‡•Ä\", 83:\"‡§§‡§ø‡§∞‡§æ‡§∏‡•Ä\", 84:\"‡§ö‡•å‡§∞‡§æ‡§∏‡•Ä\", 85:\"‡§™‡§ö‡§æ‡§∏‡•Ä\", 86:\"‡§õ‡§ø‡§Ø‡§æ‡§∏‡•Ä\", 87:\"‡§∏‡§§‡•ç‡§§‡§æ‡§∏‡•Ä\", 88:\"‡§Ö‡§ü‡•ç‡§†‡§æ‡§∏‡•Ä\", 89:\"‡§®‡§µ‡§æ‡§∏‡•Ä\",\n    90:\"‡§®‡§¨‡•ç‡§¨‡•á\", 91:\"‡§á‡§ï‡•ç‡§Ø‡§æ‡§®‡§¨‡•á\", 92:\"‡§¨‡§æ‡§®‡§µ‡•á\", 93:\"‡§§‡§ø‡§∞‡§æ‡§®‡§µ‡•á\", 94:\"‡§ö‡•å‡§∞‡§æ‡§®‡§µ‡•á\", 95:\"‡§™‡§ö‡§æ‡§®‡§µ‡•á\", 96:\"‡§õ‡§ø‡§Ø‡§æ‡§®‡§µ‡•á\", 97:\"‡§∏‡§§‡•ç‡§§‡§æ‡§®‡§µ‡•á\", 98:\"‡§Ö‡§ü‡•ç‡§†‡§æ‡§®‡§µ‡•á\", 99:\"‡§®‡§ø‡§®‡•ç‡§Ø‡§æ‡§®‡§µ‡•á\",\n    100:\"‡§∏‡•å\"\n}\n\ndef convert_numbers_to_hindi(text):\n    def replace(match):\n        num = int(match.group())\n        return hindi_numbers.get(num, match.group())  # fallback\n    \n    return re.sub(r\"\\b\\d+\\b\", replace, text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:23:08.559947Z","iopub.execute_input":"2025-12-06T14:23:08.560232Z","iopub.status.idle":"2025-12-06T14:23:08.568915Z","shell.execute_reply.started":"2025-12-06T14:23:08.560208Z","shell.execute_reply":"2025-12-06T14:23:08.568141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ncleaned = []\n\nfor f, text in metadata:\n    t = text\n\n    # convert numbers like 90 ‚Üí ‡§®‡§¨‡•ç‡§¨‡•á\n    t = convert_numbers_to_hindi(t)\n\n    # keep only Hindi chars + punctuation + spaces\n    t = re.sub(r\"[^\\u0900-\\u097F\\s?.!,']\", \"\", t)\n\n    # collapse spaces\n    t = re.sub(r\"\\s+\", \" \", t)\n\n    cleaned.append([f, t.strip()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:23:11.502253Z","iopub.execute_input":"2025-12-06T14:23:11.502545Z","iopub.status.idle":"2025-12-06T14:23:11.512079Z","shell.execute_reply.started":"2025-12-06T14:23:11.502521Z","shell.execute_reply":"2025-12-06T14:23:11.511324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"metadata.csv\", \"w\") as out:\n    for f, t in cleaned:\n        out.write(f\"wavs/{f}|{t}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:23:13.639136Z","iopub.execute_input":"2025-12-06T14:23:13.639897Z","iopub.status.idle":"2025-12-06T14:23:13.644384Z","shell.execute_reply.started":"2025-12-06T14:23:13.639869Z","shell.execute_reply":"2025-12-06T14:23:13.643786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nmkdir -p wavs_16k\n\nfor f in wavs/*.wav; do\n    ffmpeg -y -i \"$f\" -ar 16000 -ac 1 \"wavs_16k/$(basename \"$f\")\"\ndone","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import soundfile as sf\nimport os\n\nINPUT_META = \"/kaggle/working/metadata.csv\"\nAUDIO_DIR = \"/kaggle/working/wavs_16k\"\nOUTPUT_META = \"/kaggle/working/metadata_clean.csv\"\n\nclean_lines = []\n\nwith open(INPUT_META) as f:\n    for line in f:\n        wav_path, text = line.strip().split(\"|\", 1)\n\n        # 1. Remove 'wavs/' prefix\n        wav = os.path.basename(wav_path)\n\n        # 2. Skip incomplete transcripts\n        if \"...\" in text:\n            print(\"Skipping incomplete:\", text)\n            continue\n\n        # 3. Normalize multiple spaces\n        text = \" \".join(text.split())\n\n        # 4. Skip too-short transcripts (<3 words)\n        if len(text.split()) < 3:\n            print(\"Short text:\", text)\n            continue\n\n        # 5. Skip too-long transcripts (<3 words)\n        if len(text.split()) > 30:\n            print(\"long text:\", text)\n            continue\n\n        # 5. Check audio exists\n        audio_file = f\"{AUDIO_DIR}/{wav}\"\n        if not os.path.exists(audio_file):\n            print(\"Missing audio:\", wav)\n            continue\n\n        # 6. Check sample rate\n        audio, sr = sf.read(audio_file)\n        if sr != 16000:\n            print(\"Bad SR:\", wav)\n            continue\n\n        clean_lines.append(f\"{wav}|{text}\")\n\n# Write cleaned metadata\nwith open(OUTPUT_META, \"w\") as f:\n    f.write(\"\\n\".join(clean_lines))\n\nprint(\"Done. Clean samples:\", len(clean_lines))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:25:08.693365Z","iopub.execute_input":"2025-12-06T14:25:08.693987Z","iopub.status.idle":"2025-12-06T14:25:08.816063Z","shell.execute_reply.started":"2025-12-06T14:25:08.693961Z","shell.execute_reply":"2025-12-06T14:25:08.815304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Paths\naudio_dir = \"/kaggle/working/wavs_16k\"\nmetadata_files = [\"/kaggle/working/metadata_clean.csv\", \"/kaggle/working/metadata.csv\"]\noutput_zip = \"/kaggle/working/piper_dataset.zip\"\n\n# Create a temporary folder to hold all files\nimport os\ntmp_dir = \"/kaggle/working/tmp_dataset\"\nos.makedirs(tmp_dir, exist_ok=True)\n\n# Copy metadata files\nfor f in metadata_files:\n    shutil.copy(f, tmp_dir)\n\n# Copy audio folder\nshutil.copytree(audio_dir, os.path.join(tmp_dir, \"wavs_16k\"), dirs_exist_ok=True)\n\n# Zip everything\nshutil.make_archive(\"/kaggle/working/piper_dataset\", 'zip', tmp_dir)\n\nprint(\"Dataset zipped successfully! Download from: /kaggle/working/piper_dataset.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T14:25:20.554462Z","iopub.execute_input":"2025-12-06T14:25:20.555025Z","iopub.status.idle":"2025-12-06T14:25:22.674737Z","shell.execute_reply.started":"2025-12-06T14:25:20.554999Z","shell.execute_reply":"2025-12-06T14:25:22.673971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio processng ","metadata":{}},{"cell_type":"code","source":"!pip install librosa soundfile -q\nimport os\nimport librosa\nimport soundfile as sf\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:36:34.941172Z","iopub.execute_input":"2025-12-15T07:36:34.942024Z","iopub.status.idle":"2025-12-15T07:36:38.180057Z","shell.execute_reply.started":"2025-12-15T07:36:34.941985Z","shell.execute_reply":"2025-12-15T07:36:38.179339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install noisereduce -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:39:25.115267Z","iopub.execute_input":"2025-12-15T07:39:25.116044Z","iopub.status.idle":"2025-12-15T07:39:28.489791Z","shell.execute_reply.started":"2025-12-15T07:39:25.116015Z","shell.execute_reply":"2025-12-15T07:39:28.48873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import noisereduce as nr\n\ndef clean_audio(\n    input_path,\n    output_path,\n    target_sr=16000,\n    top_db=25\n):\n    audio, sr = librosa.load(input_path, sr=None, mono=True)\n\n    if sr != target_sr:\n        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n\n    # Noise reduction (light)\n    audio = nr.reduce_noise(y=audio, sr=target_sr, prop_decrease=0.8)\n\n    # Remove silence\n    audio, _ = librosa.effects.trim(audio, top_db=top_db)\n\n    # Normalize\n    audio = audio / max(abs(audio)) if max(abs(audio)) > 0 else audio\n\n    sf.write(output_path, audio, target_sr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:39:36.145871Z","iopub.execute_input":"2025-12-15T07:39:36.146548Z","iopub.status.idle":"2025-12-15T07:39:36.159297Z","shell.execute_reply.started":"2025-12-15T07:39:36.146517Z","shell.execute_reply":"2025-12-15T07:39:36.158552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_folder = \"/kaggle/input/whisper-hindi1/wavs_16k\"\noutput_folder = \"/kaggle/working/wavs_cleaned_new\"\n\nos.makedirs(output_folder, exist_ok=True)\n\nfor file in os.listdir(input_folder):\n    if not file.endswith(\".wav\"):\n        continue\n\n    in_path = os.path.join(input_folder, file)\n    out_path = os.path.join(output_folder, file)\n\n    clean_audio(in_path, out_path)\n\nprint(\"All audio files cleaned and saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:39:51.252281Z","iopub.execute_input":"2025-12-15T07:39:51.252879Z","iopub.status.idle":"2025-12-15T07:40:06.336044Z","shell.execute_reply.started":"2025-12-15T07:39:51.252852Z","shell.execute_reply":"2025-12-15T07:40:06.335222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training whisper tiny hindi","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/openai/whisper.git -q\n!pip install transformers datasets soundfile librosa evaluate -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:41:05.3474Z","iopub.execute_input":"2025-12-15T07:41:05.348042Z","iopub.status.idle":"2025-12-15T07:41:17.598472Z","shell.execute_reply.started":"2025-12-15T07:41:05.348019Z","shell.execute_reply":"2025-12-15T07:41:17.597675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q pyarrow==14.0.2 rich==13.7.1 transformers datasets==2.18.0 evaluate==0.4.1 soundfile librosa\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:41:23.85518Z","iopub.execute_input":"2025-12-15T07:41:23.855917Z","iopub.status.idle":"2025-12-15T07:41:27.404142Z","shell.execute_reply.started":"2025-12-15T07:41:23.855888Z","shell.execute_reply":"2025-12-15T07:41:27.403332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nimport pandas as pd\nimport librosa\nimport soundfile as sf\nimport os\nimport evaluate\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ---- Use WHISPER TINY ----\nmodel_name = \"openai/whisper-tiny\"  # <<<<<< IMPORTANT\n\nprocessor = WhisperProcessor.from_pretrained(\n    model_name,\n    language=\"hi\",\n    task=\"transcribe\"\n)\n\nfeature_extractor = processor.feature_extractor\ntokenizer = processor.tokenizer\n\nmodel = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)\n\n# Force Hindi\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n    language=\"hi\",\n    task=\"transcribe\"\n)\nmodel.config.suppress_tokens = []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:41:41.510346Z","iopub.execute_input":"2025-12-15T07:41:41.511214Z","iopub.status.idle":"2025-12-15T07:41:42.644654Z","shell.execute_reply.started":"2025-12-15T07:41:41.511182Z","shell.execute_reply":"2025-12-15T07:41:42.643865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# in working\n#df = pd.read_csv(\"/kaggle/working/metadata_clean.csv\", sep=\"|\", header=None)\n#df.columns = [\"file\", \"transcript\"]\n#df[\"path\"] = df[\"file\"].apply(lambda x: f\"/kaggle/working/wavs_16k/{x}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:41:45.438664Z","iopub.execute_input":"2025-12-15T07:41:45.43942Z","iopub.status.idle":"2025-12-15T07:41:45.442393Z","shell.execute_reply.started":"2025-12-15T07:41:45.439393Z","shell.execute_reply":"2025-12-15T07:41:45.441659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load metadata from the new location\ndf = pd.read_csv(\"/kaggle/input/whisper-hindi1/metadata_clean.csv\", sep=\"|\", header=None)\ndf.columns = [\"file\", \"transcript\"]\n\n# Update path to the new audio folder\nWAV_DIR = \"/kaggle/working/wavs_cleaned_new\"\ndf[\"path\"] = df[\"file\"].apply(lambda x: os.path.join(WAV_DIR, x))\n\n# Quick check\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:41:45.601527Z","iopub.execute_input":"2025-12-15T07:41:45.602188Z","iopub.status.idle":"2025-12-15T07:41:45.616051Z","shell.execute_reply.started":"2025-12-15T07:41:45.602164Z","shell.execute_reply":"2025-12-15T07:41:45.615424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = df.sample(frac=0.9, random_state=42)\nval_df   = df.drop(train_df.index)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:41:51.13893Z","iopub.execute_input":"2025-12-15T07:41:51.139221Z","iopub.status.idle":"2025-12-15T07:41:51.145341Z","shell.execute_reply.started":"2025-12-15T07:41:51.139201Z","shell.execute_reply":"2025-12-15T07:41:51.144601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HindiWhisperDataset(Dataset):\n    def __init__(self, dataframe, max_len=120):\n        self.df = dataframe\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        # --- Audio ---\n        audio, sr = sf.read(row[\"path\"])\n        if sr != 16000:\n            audio = librosa.resample(audio, sr, 16000)\n\n        input_features = feature_extractor(\n            audio,\n            sampling_rate=16000,\n            return_tensors=\"pt\"\n        ).input_features[0]\n\n        # --- Text ---\n        text = row[\"transcript\"]\n\n        tok = tokenizer(\n            text,\n            max_length=self.max_len,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n        labels = tok[\"input_ids\"]\n        labels = labels.masked_fill(tok[\"attention_mask\"] == 0, -100)\n        labels = labels[0]\n\n        return {\n            \"input_features\": input_features,\n            \"labels\": labels\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:41:51.589073Z","iopub.execute_input":"2025-12-15T07:41:51.589783Z","iopub.status.idle":"2025-12-15T07:41:51.595271Z","shell.execute_reply.started":"2025-12-15T07:41:51.589757Z","shell.execute_reply":"2025-12-15T07:41:51.594499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_set = HindiWhisperDataset(train_df)\nval_set   = HindiWhisperDataset(val_df)\n\ntrain_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:41:58.223806Z","iopub.execute_input":"2025-12-15T07:41:58.224565Z","iopub.status.idle":"2025-12-15T07:41:58.228718Z","shell.execute_reply.started":"2025-12-15T07:41:58.22453Z","shell.execute_reply":"2025-12-15T07:41:58.228008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wer_metric = evaluate.load(\"wer\")\n\ndef evaluate_model(model, val_df):\n    model.eval()\n    preds, refs = [], []\n\n    for _, row in tqdm(val_df.iterrows(), total=len(val_df)):\n        audio, sr = sf.read(row[\"path\"])\n        if sr != 16000:\n            audio = librosa.resample(audio, sr, 16000)\n\n        input_features = feature_extractor(\n            audio, sampling_rate=16000, return_tensors=\"pt\"\n        ).input_features.to(device)\n\n        with torch.no_grad():\n            pred_ids = model.generate(input_features)\n\n        pred = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0]\n        preds.append(pred)\n        refs.append(row[\"transcript\"])\n\n    return wer_metric.compute(predictions=preds, references=refs) * 100\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:41:59.199954Z","iopub.execute_input":"2025-12-15T07:41:59.200331Z","iopub.status.idle":"2025-12-15T07:41:59.611052Z","shell.execute_reply.started":"2025-12-15T07:41:59.200309Z","shell.execute_reply":"2025-12-15T07:41:59.610448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nepochs = 10\nwer_list = []\n\nfor epoch in range(epochs):\n    model.train()\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n\n    for batch in pbar:\n        input_features = batch[\"input_features\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(\n            input_features=input_features,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        pbar.set_postfix({\"loss\": loss.item()})\n\n    # Evaluate after epoch\n    W = evaluate_model(model, val_df)\n    wer_list.append(W)\n    print(f\"Epoch {epoch+1} WER = {W:.2f}%\")\n\n    plt.plot(wer_list)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"WER (%)\")\n    plt.title(\"Validation WER (Whisper-Tiny Hindi)\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:42:02.434747Z","iopub.execute_input":"2025-12-15T07:42:02.435038Z","iopub.status.idle":"2025-12-15T07:46:22.689025Z","shell.execute_reply.started":"2025-12-15T07:42:02.435017Z","shell.execute_reply":"2025-12-15T07:46:22.688372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"whisper-tiny-hindi_2\")\nprocessor.save_pretrained(\"whisper-tiny-hindi_2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T07:46:54.375016Z","iopub.execute_input":"2025-12-15T07:46:54.375303Z","iopub.status.idle":"2025-12-15T07:46:55.075209Z","shell.execute_reply.started":"2025-12-15T07:46:54.375282Z","shell.execute_reply":"2025-12-15T07:46:55.074541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Path to your saved model\nMODEL_DIR = \"whisper-tiny-hindi_2\"\nZIP_PATH = \"whisper-tiny-hindi_my.zip\"\n\n# Create a zip\nshutil.make_archive(base_name=\"whisper-tiny-hindi\", format=\"zip\", root_dir=MODEL_DIR)\n\nprint(f\"Saved zipped model at {ZIP_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:11:58.670211Z","iopub.execute_input":"2025-12-15T05:11:58.67053Z","iopub.status.idle":"2025-12-15T05:12:07.457338Z","shell.execute_reply.started":"2025-12-15T05:11:58.670507Z","shell.execute_reply":"2025-12-15T05:12:07.456461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration, WhisperProcessor\nimport torch\nimport soundfile as sf\nimport librosa\nimport os\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel_path = \"/kaggle/working/whisper-tiny-hindi\"\n\nprocessor = WhisperProcessor.from_pretrained(model_path)\ntokenizer = processor.tokenizer\nfeature_extractor = processor.feature_extractor\n\nmodel = WhisperForConditionalGeneration.from_pretrained(model_path).to(device)\nmodel.eval()\n\nprint(\"Loaded fine-tuned Whisper-tiny Hindi model!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:30:08.603069Z","iopub.execute_input":"2025-12-15T08:30:08.60337Z","iopub.status.idle":"2025-12-15T08:30:09.133621Z","shell.execute_reply.started":"2025-12-15T08:30:08.603349Z","shell.execute_reply":"2025-12-15T08:30:09.132905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transcribe_audio(path):\n    audio, sr = sf.read(path)\n\n    if sr != 16000:\n        audio = librosa.resample(audio, sr, 16000)\n\n    inputs = feature_extractor(\n        audio, sampling_rate=16000, return_tensors=\"pt\"\n    ).input_features.to(device)\n\n    with torch.no_grad():\n        predicted_ids = model.generate(\n            inputs,\n            language=\"hi\",\n            task=\"transcribe\"\n        )\n\n    text = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:30:10.882727Z","iopub.execute_input":"2025-12-15T08:30:10.883475Z","iopub.status.idle":"2025-12-15T08:30:10.88836Z","shell.execute_reply.started":"2025-12-15T08:30:10.88345Z","shell.execute_reply":"2025-12-15T08:30:10.887505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_folder = \"/kaggle/input/whisper-hindi1/wavs_16k\"\npred_results = []\n\nfor f in sorted(os.listdir(test_folder)):\n    if f.endswith(\".wav\"):\n        full_path = os.path.join(test_folder, f)\n        text = transcribe_audio(full_path)\n        pred_results.append([f, text])\n        print(f\"{f} ‚Üí {text}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:31:08.204396Z","iopub.status.idle":"2025-12-15T08:31:08.204629Z","shell.execute_reply.started":"2025-12-15T08:31:08.204519Z","shell.execute_reply":"2025-12-15T08:31:08.20453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport soundfile as sf\nimport librosa\nimport pandas as pd\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n\n# ---------------------------------------------------------------\n# LOAD FINE-TUNED MODEL\n# ---------------------------------------------------------------\nfine_tuned_path = \"/kaggle/working/whisper-tiny-hindi\"\n\nprocessor_ft = WhisperProcessor.from_pretrained(fine_tuned_path)\ntokenizer_ft = processor_ft.tokenizer\nfeature_extractor_ft = processor_ft.feature_extractor\n\nmodel_ft = WhisperForConditionalGeneration.from_pretrained(fine_tuned_path).to(device)\nmodel_ft.eval()\n\nprint(\"Loaded FINE-TUNED Whisper Tiny Hindi!\")\n\n\n# ---------------------------------------------------------------\n# LOAD ORIGINAL NON-FINE-TUNED MODEL (BASELINE)\n# ---------------------------------------------------------------\n# baseline using Whisper‚ÄëTiny‚ÄëHindi (Hindi‚Äëonly fine‚Äëtuned model)\nprocessor_base = WhisperProcessor.from_pretrained(\"collabora/whisper-tiny-hindi\")\ntokenizer_base = processor_base.tokenizer\nfeature_extractor_base = processor_base.feature_extractor\n\nmodel_base = WhisperForConditionalGeneration.from_pretrained(\"collabora/whisper-tiny-hindi\").to(device)\nmodel_base.eval()\n\nprint(\"Loaded BASELINE Whisper Tiny Hindi (non fine‚Äëtuned)\")\n\n\n\n# ---------------------------------------------------------------\n# COMMON TRANSCRIPTION FUNCTION\n# ---------------------------------------------------------------\ndef transcribe(path, model, processor, tokenizer):\n    audio, sr = sf.read(path)\n\n    if sr != 16000:\n        audio = librosa.resample(audio, sr, 16000)\n\n    inputs = processor.feature_extractor(\n        audio,\n        sampling_rate=16000,\n        return_tensors=\"pt\"\n    ).input_features.to(device)\n\n    with torch.no_grad():\n        predicted_ids = model.generate(\n            inputs,\n            language=\"hi\",\n            task=\"transcribe\"\n        )\n\n    text = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n    return text\n\n\n# ---------------------------------------------------------------\n# RUN BOTH MODELS ON TEST SET\n# ---------------------------------------------------------------\ntest_folder = \"/kaggle/input/whisper-hindi1/wavs_16k\"\n\nresults = []\n\nprint(\"\\n========= Running Predictions =========\\n\")\n\nfor idx, f in enumerate(sorted(os.listdir(test_folder))):\n    if not f.endswith(\".wav\"):\n        continue\n\n    full_path = os.path.join(test_folder, f)\n\n    pred_ft = transcribe(full_path, model_ft, processor_ft, tokenizer_ft)\n    pred_base = transcribe(full_path, model_base, processor_base, tokenizer_base)\n\n    # üîπ PRINT ONLY FIRST 50 FILES\n    if idx < 50:\n        print(f\"\\nFile: {f}\")\n        print(\"Fine-tuned ‚Üí \", pred_ft)\n        print(\"Baseline   ‚Üí \", pred_base)\n\n    # üîπ SAVE ALL FILES\n    results.append([f, pred_ft, pred_base])\n\n\n\n# ---------------------------------------------------------------\n# SAVE COMPARISON CSV\n# ---------------------------------------------------------------\ndf = pd.DataFrame(results, columns=[\"filename\", \"fine_tuned\", \"baseline\"])\ndf.to_csv(\"/kaggle/working/whisper_hindi_comparison.csv\", index=False)\n\nprint(\"\\nSaved comparison CSV at: /kaggle/working/whisper_hindi_comparison.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:31:52.704204Z","iopub.execute_input":"2025-12-15T08:31:52.704767Z","iopub.status.idle":"2025-12-15T08:35:47.301711Z","shell.execute_reply.started":"2025-12-15T08:31:52.704745Z","shell.execute_reply":"2025-12-15T08:35:47.300944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Colabora model finetuning","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport librosa\nimport soundfile as sf\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:35:47.302788Z","iopub.execute_input":"2025-12-15T08:35:47.303029Z","iopub.status.idle":"2025-12-15T08:35:47.306357Z","shell.execute_reply.started":"2025-12-15T08:35:47.303012Z","shell.execute_reply":"2025-12-15T08:35:47.305685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:35:47.307056Z","iopub.execute_input":"2025-12-15T08:35:47.307203Z","iopub.status.idle":"2025-12-15T08:35:47.310308Z","shell.execute_reply.started":"2025-12-15T08:35:47.30719Z","shell.execute_reply":"2025-12-15T08:35:47.309777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"collabora/whisper-tiny-hindi\"\n\nprocessor = WhisperProcessor.from_pretrained(\n    model_name,\n    language=\"hi\",\n    task=\"transcribe\"\n)\n\nmodel = WhisperForConditionalGeneration.from_pretrained(model_name)\nmodel.to(device)\n\n# Force Hindi decoding\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n    language=\"hi\",\n    task=\"transcribe\"\n)\nmodel.config.suppress_tokens = []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:35:47.312239Z","iopub.execute_input":"2025-12-15T08:35:47.312429Z","iopub.status.idle":"2025-12-15T08:35:48.407666Z","shell.execute_reply.started":"2025-12-15T08:35:47.312414Z","shell.execute_reply":"2025-12-15T08:35:48.407082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HindiASRDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        audio, sr = sf.read(row[\"path\"])\n\n        if sr != 16000:\n            audio = librosa.resample(audio, sr, 16000)\n\n        input_features = processor.feature_extractor(\n            audio,\n            sampling_rate=16000,\n            return_tensors=\"pt\"\n        ).input_features[0]\n\n        labels = processor.tokenizer(\n            row[\"transcript\"],\n            return_tensors=\"pt\"\n        ).input_ids[0]\n\n        return {\n            \"input_features\": input_features,\n            \"labels\": labels\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:31:08.21116Z","iopub.status.idle":"2025-12-15T08:31:08.211366Z","shell.execute_reply.started":"2025-12-15T08:31:08.211264Z","shell.execute_reply":"2025-12-15T08:31:08.211274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    input_features = torch.stack(\n        [item[\"input_features\"] for item in batch]\n    )\n\n    labels = [item[\"labels\"] for item in batch]\n    labels = pad_sequence(\n        labels,\n        batch_first=True,\n        padding_value=processor.tokenizer.pad_token_id\n    )\n\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n\n    return {\n        \"input_features\": input_features,\n        \"labels\": labels\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:31:08.21333Z","iopub.status.idle":"2025-12-15T08:31:08.213718Z","shell.execute_reply.started":"2025-12-15T08:31:08.213509Z","shell.execute_reply":"2025-12-15T08:31:08.213527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = HindiWhisperDataset(train_df)\nval_dataset   = HindiWhisperDataset(val_df)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=8,      # safer for Kaggle\n    shuffle=True,\n    collate_fn=collate_fn\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:31:08.215024Z","iopub.status.idle":"2025-12-15T08:31:08.215253Z","shell.execute_reply.started":"2025-12-15T08:31:08.215145Z","shell.execute_reply":"2025-12-15T08:31:08.215156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nepochs = 10\nwer_list = []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:31:08.216071Z","iopub.status.idle":"2025-12-15T08:31:08.216278Z","shell.execute_reply.started":"2025-12-15T08:31:08.216175Z","shell.execute_reply":"2025-12-15T08:31:08.216184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(epochs):\n    model.train()\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n\n    for batch in pbar:\n        input_features = batch[\"input_features\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(\n            input_features=input_features,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        pbar.set_postfix({\"loss\": loss.item()})\n\n    # -------- VALIDATION (WER) --------\n    model.eval()\n    W = evaluate_model(model, val_df)   # SAME FUNCTION AS BEFORE\n    wer_list.append(W)\n\n    print(f\"Epoch {epoch+1} WER = {W:.2f}%\")\n\n    plt.figure()\n    plt.plot(range(1, len(wer_list) + 1), wer_list, marker=\"o\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"WER (%)\")\n    plt.title(\"Validation WER (Collabora Whisper-Tiny Hindi)\")\n    plt.grid(True)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:31:08.217563Z","iopub.status.idle":"2025-12-15T08:31:08.217844Z","shell.execute_reply.started":"2025-12-15T08:31:08.217698Z","shell.execute_reply":"2025-12-15T08:31:08.217712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = \"/kaggle/working/collabora-whisper-tiny-hindi-ft\"\n\nmodel.save_pretrained(save_path)\nprocessor.save_pretrained(save_path)\n\nprint(\"Model saved at:\", save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:03:00.707139Z","iopub.execute_input":"2025-12-15T08:03:00.707433Z","iopub.status.idle":"2025-12-15T08:03:01.290447Z","shell.execute_reply.started":"2025-12-15T08:03:00.707411Z","shell.execute_reply":"2025-12-15T08:03:01.289681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Path to your saved model\nMODEL_DIR = \"/kaggle/working/collabora-whisper-tiny-hindi-ft\"\nZIP_PATH = \"/kaggle/working/collabora-whisper-tiny-hindi-ft.zip\"\n\n# Create a zip\nshutil.make_archive(base_name=\"/kaggle/working/collabora-whisper-tiny-hindi-ft\", format=\"zip\", root_dir=MODEL_DIR)\n\nprint(f\"Saved zipped model at {ZIP_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T05:53:27.759958Z","iopub.execute_input":"2025-12-15T05:53:27.76047Z","iopub.status.idle":"2025-12-15T05:53:35.892963Z","shell.execute_reply.started":"2025-12-15T05:53:27.760446Z","shell.execute_reply":"2025-12-15T05:53:35.892209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport librosa\nimport soundfile as sf\nimport pandas as pd\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\n\nft_path = \"/kaggle/working/collabora-whisper-tiny-hindi-ft\"\n\nprocessor_ft = WhisperProcessor.from_pretrained(ft_path)\nmodel_ft = WhisperForConditionalGeneration.from_pretrained(ft_path).to(device)\nmodel_ft.eval()\n\n# Force Hindi\nmodel_ft.config.forced_decoder_ids = processor_ft.get_decoder_prompt_ids(\n    language=\"hi\",\n    task=\"transcribe\"\n)\nmodel_ft.config.suppress_tokens = []\n\nprint(\"Loaded FINE-TUNED Collabora model\")\n\nbase_model_name = \"collabora/whisper-tiny-hindi\"\n\nprocessor_base = WhisperProcessor.from_pretrained(\n    base_model_name,\n    language=\"hi\",\n    task=\"transcribe\"\n)\n\nmodel_base = WhisperForConditionalGeneration.from_pretrained(\n    base_model_name\n).to(device)\nmodel_base.eval()\n\n# Force Hindi\nmodel_base.config.forced_decoder_ids = processor_base.get_decoder_prompt_ids(\n    language=\"hi\",\n    task=\"transcribe\"\n)\nmodel_base.config.suppress_tokens = []\n\nprint(\"Loaded BASELINE Collabora model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:35:48.408385Z","iopub.execute_input":"2025-12-15T08:35:48.408605Z","iopub.status.idle":"2025-12-15T08:35:49.940047Z","shell.execute_reply.started":"2025-12-15T08:35:48.408587Z","shell.execute_reply":"2025-12-15T08:35:49.939278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transcribe(audio_path, model, processor):\n    audio, sr = sf.read(audio_path)\n\n    if sr != 16000:\n        audio = librosa.resample(audio, sr, 16000)\n\n    inputs = processor.feature_extractor(\n        audio,\n        sampling_rate=16000,\n        return_tensors=\"pt\"\n    ).input_features.to(device)\n\n    with torch.no_grad():\n        predicted_ids = model.generate(inputs)\n\n    text = processor.tokenizer.batch_decode(\n        predicted_ids,\n        skip_special_tokens=True\n    )[0]\n\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:35:49.94092Z","iopub.execute_input":"2025-12-15T08:35:49.941217Z","iopub.status.idle":"2025-12-15T08:35:49.946396Z","shell.execute_reply.started":"2025-12-15T08:35:49.941192Z","shell.execute_reply":"2025-12-15T08:35:49.945701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_folder = \"/kaggle/input/whisper-hindi1/wavs_16k\"\n\nresults = []\n\nprint(\"\\n===== Running Collabora Model Comparison =====\\n\")\n\nfiles = sorted(os.listdir(test_folder))\n\nfor idx, file in enumerate(tqdm(files, desc=\"Transcribing audio files\")):\n    if not file.endswith(\".wav\"):\n        continue\n\n    path = os.path.join(test_folder, file)\n\n    pred_ft = transcribe(path, model_ft, processor_ft)\n    pred_base = transcribe(path, model_base, processor_base)\n\n    # üîπ PRINT ONLY FIRST 50 FILES\n    if idx < 5:\n        print(f\"\\nFile: {file}\")\n        print(\"Fine-tuned :\", pred_ft)\n        print(\"Baseline  :\", pred_base)\n\n    # üîπ SAVE ALL FILES\n    results.append([file, pred_ft, pred_base])\n\n\ndf = pd.DataFrame(\n    results,\n    columns=[\"filename\", \"fine_tuned_collabora\", \"baseline_collabora\"]\n)\n\ncsv_path = \"/kaggle/working/collabora_finetune_vs_baseline.csv\"\ndf.to_csv(csv_path, index=False)\n\nprint(\"\\nSaved comparison CSV at:\", csv_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:35:49.947031Z","iopub.execute_input":"2025-12-15T08:35:49.947226Z","iopub.status.idle":"2025-12-15T08:42:18.4313Z","shell.execute_reply.started":"2025-12-15T08:35:49.947212Z","shell.execute_reply":"2025-12-15T08:42:18.430666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Compute WER from CSV","metadata":{}},{"cell_type":"code","source":"!pip install jiwer -q\nimport pandas as pd\nfrom jiwer import wer\nimport re\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:18.432072Z","iopub.execute_input":"2025-12-15T08:42:18.432251Z","iopub.status.idle":"2025-12-15T08:42:21.598252Z","shell.execute_reply.started":"2025-12-15T08:42:18.432235Z","shell.execute_reply":"2025-12-15T08:42:21.597332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_df = pd.read_csv(\"/kaggle/working/collabora_finetune_vs_baseline.csv\")\nref_df  = pd.read_csv(\"/kaggle/input/metadata-clean-header/metadata-clean-header.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.599474Z","iopub.execute_input":"2025-12-15T08:42:21.59977Z","iopub.status.idle":"2025-12-15T08:42:21.618281Z","shell.execute_reply.started":"2025-12-15T08:42:21.599746Z","shell.execute_reply":"2025-12-15T08:42:21.617675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(pred_df.columns)\nprint(ref_df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.621365Z","iopub.execute_input":"2025-12-15T08:42:21.6216Z","iopub.status.idle":"2025-12-15T08:42:21.626104Z","shell.execute_reply.started":"2025-12-15T08:42:21.621582Z","shell.execute_reply":"2025-12-15T08:42:21.625287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ref_df[\"filename\"] = ref_df[\"Audio_no\"]\nref_df = ref_df.rename(columns={\"Transcript\": \"reference\"})\nref_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.626924Z","iopub.execute_input":"2025-12-15T08:42:21.627188Z","iopub.status.idle":"2025-12-15T08:42:21.637948Z","shell.execute_reply.started":"2025-12-15T08:42:21.627163Z","shell.execute_reply":"2025-12-15T08:42:21.637221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pred_df.merge(\n    ref_df[[\"filename\", \"reference\"]],\n    on=\"filename\",\n    how=\"inner\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.638829Z","iopub.execute_input":"2025-12-15T08:42:21.639543Z","iopub.status.idle":"2025-12-15T08:42:21.646894Z","shell.execute_reply.started":"2025-12-15T08:42:21.639518Z","shell.execute_reply":"2025-12-15T08:42:21.646073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.647477Z","iopub.execute_input":"2025-12-15T08:42:21.647669Z","iopub.status.idle":"2025-12-15T08:42:21.655344Z","shell.execute_reply.started":"2025-12-15T08:42:21.647627Z","shell.execute_reply":"2025-12-15T08:42:21.654596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize(text):\n    text = str(text).lower()\n    text = re.sub(r\"[^\\u0900-\\u097F\\s]\", \"\", text)  # keep Hindi only\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.656948Z","iopub.execute_input":"2025-12-15T08:42:21.6572Z","iopub.status.idle":"2025-12-15T08:42:21.661751Z","shell.execute_reply.started":"2025-12-15T08:42:21.657183Z","shell.execute_reply":"2025-12-15T08:42:21.661033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"ref_norm\"] = df[\"reference\"].apply(normalize)\ndf[\"baseline_norm\"] = df[\"baseline_collabora\"].apply(normalize)\ndf[\"finetuned_norm\"] = df[\"fine_tuned_collabora\"].apply(normalize)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.662567Z","iopub.execute_input":"2025-12-15T08:42:21.66325Z","iopub.status.idle":"2025-12-15T08:42:21.678194Z","shell.execute_reply.started":"2025-12-15T08:42:21.663226Z","shell.execute_reply":"2025-12-15T08:42:21.677616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.678836Z","iopub.execute_input":"2025-12-15T08:42:21.679016Z","iopub.status.idle":"2025-12-15T08:42:21.687441Z","shell.execute_reply.started":"2025-12-15T08:42:21.679001Z","shell.execute_reply":"2025-12-15T08:42:21.686769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"baseline_wer\"] = df.apply(\n    lambda x: wer(x[\"ref_norm\"], x[\"baseline_norm\"]),\n    axis=1\n)\n\ndf[\"finetuned_wer\"] = df.apply(\n    lambda x: wer(x[\"ref_norm\"], x[\"finetuned_norm\"]),\n    axis=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.6882Z","iopub.execute_input":"2025-12-15T08:42:21.688404Z","iopub.status.idle":"2025-12-15T08:42:21.728025Z","shell.execute_reply.started":"2025-12-15T08:42:21.688389Z","shell.execute_reply":"2025-12-15T08:42:21.727277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_avg = df[\"baseline_wer\"].mean() * 100\nfinetuned_avg = df[\"finetuned_wer\"].mean() * 100\n\nprint(f\"Baseline WER   : {baseline_avg:.2f}%\")\nprint(f\"Fine-tuned WER : {finetuned_avg:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.728735Z","iopub.execute_input":"2025-12-15T08:42:21.728936Z","iopub.status.idle":"2025-12-15T08:42:21.733445Z","shell.execute_reply.started":"2025-12-15T08:42:21.728921Z","shell.execute_reply":"2025-12-15T08:42:21.73267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"improved\"] = df[\"finetuned_wer\"] < df[\"baseline_wer\"]\n\nprint(\"Improved files:\", df[\"improved\"].sum())\nprint(\"Total files   :\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.734327Z","iopub.execute_input":"2025-12-15T08:42:21.734614Z","iopub.status.idle":"2025-12-15T08:42:21.739155Z","shell.execute_reply.started":"2025-12-15T08:42:21.734599Z","shell.execute_reply":"2025-12-15T08:42:21.738506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_csv = \"/kaggle/working/collabora_wer_evaluation.csv\"\ndf.to_csv(final_csv, index=False)\n\nprint(\"Saved full evaluation CSV at:\", final_csv)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.739798Z","iopub.execute_input":"2025-12-15T08:42:21.739988Z","iopub.status.idle":"2025-12-15T08:42:21.75117Z","shell.execute_reply.started":"2025-12-15T08:42:21.739974Z","shell.execute_reply":"2025-12-15T08:42:21.750625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"improved_pct = (df[\"improved\"].sum() / len(df)) * 100\nprint(f\"Fine-tuned model performed better on {improved_pct:.2f}% of files\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.751791Z","iopub.execute_input":"2025-12-15T08:42:21.751949Z","iopub.status.idle":"2025-12-15T08:42:21.755648Z","shell.execute_reply.started":"2025-12-15T08:42:21.751936Z","shell.execute_reply":"2025-12-15T08:42:21.754896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[df[\"ref_norm\"].str.len() > 0]\ndf = df[df[\"ref_norm\"].str.split().apply(len) >= 3]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.756278Z","iopub.execute_input":"2025-12-15T08:42:21.75645Z","iopub.status.idle":"2025-12-15T08:42:21.763766Z","shell.execute_reply.started":"2025-12-15T08:42:21.756436Z","shell.execute_reply":"2025-12-15T08:42:21.763194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.sort_values(\"finetuned_wer\", ascending=False).head(49)[\n    [\"filename\", \"reference\", \"baseline_collabora\", \"fine_tuned_collabora\",\n     \"baseline_wer\", \"finetuned_wer\"]\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.764397Z","iopub.execute_input":"2025-12-15T08:42:21.764581Z","iopub.status.idle":"2025-12-15T08:42:21.77939Z","shell.execute_reply.started":"2025-12-15T08:42:21.764567Z","shell.execute_reply":"2025-12-15T08:42:21.778681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"baseline_wer\"].mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.780064Z","iopub.execute_input":"2025-12-15T08:42:21.780224Z","iopub.status.idle":"2025-12-15T08:42:21.784817Z","shell.execute_reply.started":"2025-12-15T08:42:21.780211Z","shell.execute_reply":"2025-12-15T08:42:21.783947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nbaseline_corpus_wer = wer(\n    list(df[\"ref_norm\"]),\n    list(df[\"baseline_norm\"])\n) * 100\n\nfinetuned_corpus_wer = wer(\n    list(df[\"ref_norm\"]),\n    list(df[\"finetuned_norm\"])\n) * 100\n\nprint(f\"Baseline CORPUS WER   : {baseline_corpus_wer:.2f}%\")\nprint(f\"Fine-tuned CORPUS WER : {finetuned_corpus_wer:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.785501Z","iopub.execute_input":"2025-12-15T08:42:21.785739Z","iopub.status.idle":"2025-12-15T08:42:21.806909Z","shell.execute_reply.started":"2025-12-15T08:42:21.785723Z","shell.execute_reply":"2025-12-15T08:42:21.806318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Compare Wishper Fine Tune and Collabora Fine tune","metadata":{}},{"cell_type":"code","source":"# Whisper fine-tuned predictions\nwhisper_df = pd.read_csv(\"/kaggle/working/whisper_hindi_comparison.csv\")\n\n# Collabora fine-tuned predictions\ncollabora_df = pd.read_csv(\"/kaggle/working/collabora_finetune_vs_baseline.csv\")\n\n# Reference\nref_df = pd.read_csv(\"/kaggle/input/metadata-clean-header/metadata-clean-header.csv\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.807684Z","iopub.execute_input":"2025-12-15T08:42:21.80839Z","iopub.status.idle":"2025-12-15T08:42:21.823037Z","shell.execute_reply.started":"2025-12-15T08:42:21.808364Z","shell.execute_reply":"2025-12-15T08:42:21.822498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ref_df[\"filename\"] = ref_df[\"Audio_no\"]\n\nref_df = ref_df.rename(columns={\"Transcript\": \"reference\"})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.823743Z","iopub.execute_input":"2025-12-15T08:42:21.823938Z","iopub.status.idle":"2025-12-15T08:42:21.828344Z","shell.execute_reply.started":"2025-12-15T08:42:21.823923Z","shell.execute_reply":"2025-12-15T08:42:21.827778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"whisper_df = whisper_df[[\"filename\", \"fine_tuned\"]]\ncollabora_df = collabora_df[[\"filename\", \"fine_tuned_collabora\"]]\nref_df = ref_df[[\"filename\", \"reference\"]]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.831989Z","iopub.execute_input":"2025-12-15T08:42:21.832229Z","iopub.status.idle":"2025-12-15T08:42:21.837076Z","shell.execute_reply.started":"2025-12-15T08:42:21.832213Z","shell.execute_reply":"2025-12-15T08:42:21.836455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = ref_df \\\n    .merge(whisper_df, on=\"filename\", how=\"inner\") \\\n    .merge(collabora_df, on=\"filename\", how=\"inner\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.837708Z","iopub.execute_input":"2025-12-15T08:42:21.837895Z","iopub.status.idle":"2025-12-15T08:42:21.845204Z","shell.execute_reply.started":"2025-12-15T08:42:21.83788Z","shell.execute_reply":"2025-12-15T08:42:21.844583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.845819Z","iopub.execute_input":"2025-12-15T08:42:21.846007Z","iopub.status.idle":"2025-12-15T08:42:21.852789Z","shell.execute_reply.started":"2025-12-15T08:42:21.845992Z","shell.execute_reply":"2025-12-15T08:42:21.852256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef normalize(text):\n    text = str(text).lower()\n\n    # Remove Hindi danda and punctuation\n    text = re.sub(r\"[‡•§|,!?;:]\", \"\", text)\n\n    # Keep only Hindi characters and spaces\n    text = re.sub(r\"[^\\u0900-\\u097F\\s]\", \"\", text)\n\n    # Remove common fillers (optional but recommended)\n    text = re.sub(r\"\\b(‡§π‡§æ‡§Ç|‡§π‡§æ‡§Å|‡§Ö|‡§Ö‡§Ç|‡§π‡•Ç‡§Å|‡§π‡•à)\\b\", \"\", text)\n\n    # Normalize spaces\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    return text.strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.853423Z","iopub.execute_input":"2025-12-15T08:42:21.85361Z","iopub.status.idle":"2025-12-15T08:42:21.857464Z","shell.execute_reply.started":"2025-12-15T08:42:21.853595Z","shell.execute_reply":"2025-12-15T08:42:21.856865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"ref_norm\"] = df[\"reference\"].apply(normalize)\ndf[\"whisper_norm\"] = df[\"fine_tuned\"].apply(normalize)\ndf[\"collabora_norm\"] = df[\"fine_tuned_collabora\"].apply(normalize)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.858169Z","iopub.execute_input":"2025-12-15T08:42:21.858375Z","iopub.status.idle":"2025-12-15T08:42:21.876626Z","shell.execute_reply.started":"2025-12-15T08:42:21.858359Z","shell.execute_reply":"2025-12-15T08:42:21.876089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[df[\"ref_norm\"].str.split().apply(len) >= 3]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.877293Z","iopub.execute_input":"2025-12-15T08:42:21.877455Z","iopub.status.idle":"2025-12-15T08:42:21.882596Z","shell.execute_reply.started":"2025-12-15T08:42:21.877442Z","shell.execute_reply":"2025-12-15T08:42:21.881915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"whisper_corpus_wer = wer(\n    list(df[\"ref_norm\"]),\n    list(df[\"whisper_norm\"])\n) * 100\n\ncollabora_corpus_wer = wer(\n    list(df[\"ref_norm\"]),\n    list(df[\"collabora_norm\"])\n) * 100\n\nprint(f\"Whisper Fine-tuned WER    : {whisper_corpus_wer:.2f}%\")\nprint(f\"Collabora Fine-tuned WER  : {collabora_corpus_wer:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.88333Z","iopub.execute_input":"2025-12-15T08:42:21.883864Z","iopub.status.idle":"2025-12-15T08:42:21.903306Z","shell.execute_reply.started":"2025-12-15T08:42:21.88384Z","shell.execute_reply":"2025-12-15T08:42:21.902626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.90402Z","iopub.execute_input":"2025-12-15T08:42:21.904317Z","iopub.status.idle":"2025-12-15T08:42:21.912821Z","shell.execute_reply.started":"2025-12-15T08:42:21.904285Z","shell.execute_reply":"2025-12-15T08:42:21.912143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def truncate_to_ref(pred, ref):\n    pred_words = pred.split()\n    ref_len = len(ref.split())\n    return \" \".join(pred_words[:ref_len])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.913537Z","iopub.execute_input":"2025-12-15T08:42:21.91374Z","iopub.status.idle":"2025-12-15T08:42:21.917139Z","shell.execute_reply.started":"2025-12-15T08:42:21.913725Z","shell.execute_reply":"2025-12-15T08:42:21.916403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"collabora_trunc\"] = df.apply(\n    lambda x: truncate_to_ref(x[\"collabora_norm\"], x[\"ref_norm\"]),\n    axis=1\n)\n\ndf[\"whisper_trunc\"] = df.apply(\n    lambda x: truncate_to_ref(x[\"whisper_norm\"], x[\"ref_norm\"]),\n    axis=1\n)\n\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.918677Z","iopub.execute_input":"2025-12-15T08:42:21.918888Z","iopub.status.idle":"2025-12-15T08:42:21.936281Z","shell.execute_reply.started":"2025-12-15T08:42:21.918873Z","shell.execute_reply":"2025-12-15T08:42:21.935695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\nwhisper_corpus_wer = wer(\n    list(df[\"ref_norm\"]),\n    list(df[\"whisper_trunc\"])\n) * 100\n\ncollabora_corpus_wer = wer(\n    list(df[\"ref_norm\"]),\n    list(df[\"collabora_trunc\"])\n) * 100\n\nprint(f\"Whisper Fine-tuned CORPUS WER    : {whisper_corpus_wer:.2f}%\")\nprint(f\"Collabora Fine-tuned CORPUS WER  : {collabora_corpus_wer:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.936959Z","iopub.execute_input":"2025-12-15T08:42:21.937143Z","iopub.status.idle":"2025-12-15T08:42:21.953724Z","shell.execute_reply.started":"2025-12-15T08:42:21.937129Z","shell.execute_reply":"2025-12-15T08:42:21.953169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.sort_values(\"collabora_norm\", ascending=False).head(3)[\n    [\"reference\", \"whisper_norm\", \"collabora_norm\"]\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.954347Z","iopub.execute_input":"2025-12-15T08:42:21.954525Z","iopub.status.idle":"2025-12-15T08:42:21.963016Z","shell.execute_reply.started":"2025-12-15T08:42:21.954511Z","shell.execute_reply":"2025-12-15T08:42:21.962264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PostProcessing for removing Over generation","metadata":{}},{"cell_type":"code","source":"def remove_fillers(text):\n    fillers = [\n        \"‡§î‡§∞ ‡§Æ‡•à‡§Ç\",\n        \"‡§î‡§∞ ‡§´‡§ø‡§∞\",\n        \"‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç\",\n        \"‡§á‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶\",\n        \"‡§Ø‡§π‡§æ‡§Å ‡§™‡§∞\",\n        \"‡§á‡§∏ ‡§§‡§∞‡§π ‡§∏‡•á\"\n    ]\n    for f in fillers:\n        text = text.split(f)[0]\n    return text.strip()\ndf[\"collabora_pp\"] = df[\"collabora_norm\"].apply(remove_fillers)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.963893Z","iopub.execute_input":"2025-12-15T08:42:21.964321Z","iopub.status.idle":"2025-12-15T08:42:21.969192Z","shell.execute_reply.started":"2025-12-15T08:42:21.964301Z","shell.execute_reply":"2025-12-15T08:42:21.96846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stop_on_repetition(text):\n    words = text.split()\n    cleaned = []\n    for w in words:\n        if cleaned.count(w) > 1:\n            break\n        cleaned.append(w)\n    return \" \".join(cleaned)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.969872Z","iopub.execute_input":"2025-12-15T08:42:21.970054Z","iopub.status.idle":"2025-12-15T08:42:21.974188Z","shell.execute_reply.started":"2025-12-15T08:42:21.97004Z","shell.execute_reply":"2025-12-15T08:42:21.973657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def truncate_to_reference(pred, ref):\n    pred_words = pred.split()\n    ref_len = len(ref.split())\n    return \" \".join(pred_words[:ref_len])\n\ndf[\"collabora_pp\"] = df.apply(\n    lambda x: truncate_to_reference(x[\"collabora_norm\"], x[\"ref_norm\"]),\n    axis=1\n)\n\n\ndef post_process(pred, ref=None):\n    pred = normalize(pred)\n    pred = remove_fillers(pred)\n\n    if ref is not None:\n        pred = truncate_to_reference(pred, ref)\n\n    return pred\ndf[\"collabora_pp\"] = df.apply(\n    lambda x: post_process(x[\"collabora_norm\"], x[\"ref_norm\"]),\n    axis=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.974812Z","iopub.execute_input":"2025-12-15T08:42:21.974993Z","iopub.status.idle":"2025-12-15T08:42:21.994631Z","shell.execute_reply.started":"2025-12-15T08:42:21.974979Z","shell.execute_reply":"2025-12-15T08:42:21.993959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from jiwer import wer\n\ncollabora_pp_wer = wer(\n    list(df[\"ref_norm\"]),\n    list(df[\"collabora_pp\"])\n) * 100\n\nprint(f\"Collabora Post-Processed CORPUS WER: {collabora_pp_wer:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T08:42:21.9954Z","iopub.execute_input":"2025-12-15T08:42:21.995738Z","iopub.status.idle":"2025-12-15T08:42:22.006263Z","shell.execute_reply.started":"2025-12-15T08:42:21.995712Z","shell.execute_reply":"2025-12-15T08:42:22.005528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}